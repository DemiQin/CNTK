# Node: ResNet-50 with ImageNet, with original paper setting

command = TrainNetwork:BNStatistics:Eval

precision = "float"; traceLevel = 1 ; deviceId = "auto"

rootDir = "." ; configDir = "$RootDir$" ; dataDir = "$RootDir$" ;
outputDir = "$rootDir$/Output" ;

modelPath = "$outputDir$/Models/ResNet_50"
stderr = "$outputDir$/ResNet_50_BS_out"

parallelTrain = "true"

TrainNetwork = {
    action = "train"

    BrainScriptNetworkBuilder = {
        include "$configDir$/Macros.bs"

        imageShape  = 224:224:3                 # image dimensions
        labelDim    = 1000                      # number of distinct labels

        cMap        = 64:128:256:512:1024:2048  
        numLayers   = 2:3:5:2
        bnTimeConst = 0

        # stride in BottleneckInc
        stride1*1 = (1:1)
        stride3*3 = (3:3)

        model = Sequential(
            # conv1 and max pooling
            ConvBNReLULayer {cMap[0], (7:7), (2:2), bnTimeConst} :
            MaxPoolingLayer {(3:3), stride = 2, pad = true} :

            # conv2_x
            ResNetBottleneckInc {cMap[2], cMap[0], stride1*1, stride3*3, bnTimeConst} :
            ResNetBottleneckStack {numLayers[0], cMap[2], cMap[0], bnTimeConst} :

            # conv3_x
            ResNetBottleneckInc {cMap[3], cMap[1], stride1*1, stride3*3, bnTimeConst} :
            ResNetBottleneckStack {numLayers[1], cMap[3], cMap[1], bnTimeConst} :

            # conv4_x
            ResNetBottleneckInc {cMap[4], cMap[2], stride1*1, stride3*3, bnTimeConst} :
            ResNetBottleneckStack {numLayers[2], cMap[4], cMap[2], bnTimeConst} :

            # conv5_x
            ResNetBottleneckInc {cMap[5], cMap[3], stride1*1, stride3*3, bnTimeConst} :
            ResNetBottleneckStack {numLayers[3], cMap[5], cMap[3], bnTimeConst} :

            # avg pooling
            AveragePoolingLayer {(7: 7), stride = 1} :

            # FC
            LinearLayer {labelDim, init= 'uniform'}
        )

        # inputs
        features    = Input {imageShape}
        labels      = Input {labelDim}

        # apply model to features
        ol          = model (features)

        # loss and error computation
        ce          = CrossEntropyWithSoftmax   (labels, ol)
        errs        = ClassificationError       (labels, ol)
        top5Errs    = ClassificationError       (labels, ol, topN = 5)

        # declare special nodes
        featureNodes    = (features)
        labelNodes      = (labels)
        criterionNodes  = (ce)
        evaluationNodes = (errs) # top5Errs only used in Eval
        outputNodes     = (ol)
    }

    SGD = {
        epochSize = 0
        minibatchSize = 256
        maxEpochs = 125
        learningRatesPerMB = 1*30: 0.1*30: 0.01*30: 0.001
        momentumPerMB = 0.9
        gradUpdateType = "None"
        L2RegWeight = 0.0001
        dropoutRate = 0
        numMBsToShowResult = 500

        disableWkInBatchNormal = true

        ParallelTrain = {
            parallelizationMethod = "DataParallelSGD"
            distributedMBReading = "true"
            parallelizationStartEpoch = 1
            DataParallelSGD = {
                gradientBits = 32
            }
        }
    }

    reader = {
        verbosity = 0 ; randomize = true
        deserializers = ({
            type = "ImageDeserializer" ; module = "ImageReader"
            file = "$dataDir$/train_map.txt"
            input = {
                features = { transforms = (
                    { type = "Crop" ; cropType = "random" ; cropRatio = 0.46666:0.875 ; jitterType = "uniRatio" } :
                    { type = "Scale" ; width = 224 ; height = 224 ; channels = 3 ; interpolations = "linear" } :
                    { type = "Mean" ; meanFile = "$ConfigDir$/ImageNet1K_mean.xml" } : 
                    { type = "Transpose" }
                )}
                labels = { labelDim = 1000 }
            }
        })
    }
}


BNStatistics = {
    action = "pbn"
    modelPath = "$modelPath$"
    minibatchSize = 256
    iters = 30

    reader = {
        verbosity = 0 ; randomize = true
        deserializers = ({
            type = "ImageDeserializer" ; module = "ImageReader"
            file = "$dataDir$/train_map.txt"
            input = {
                features = { transforms = (
                    { type = "Crop" ; cropType = "random" ; cropRatio = 0.46666:0.875 ; jitterType = "uniRatio" } :
                    { type = "Scale" ; width = 224 ; height = 224 ; channels = 3 ; interpolations = "linear" } :
                    { type = "Mean" ; meanFile = "$ConfigDir$/ImageNet1K_mean.xml" } : 
                    { type = "Transpose" }
                )}
                labels = { labelDim = 1000 }
            }
        })
    }
}

Eval = {
    action = "test"
    modelPath = "$modelPath$.PBN"
    # Set minibatch size for testing.
    minibatchSize = 100
    evalNodeNames = errs:top5Errs

    reader = {
        verbosity = 0 ; randomize = false
        deserializers = ({
            type = "ImageDeserializer" ; module = "ImageReader"
            file = "$dataDir$/train_map.txt"
            input = {
                features = { transforms = (
                    { type = "Crop" ; cropType = "center" ; cropRatio = 0.875 ; jitterType = "uniRatio" } :
                    { type = "Scale" ; width = 224 ; height = 224 ; channels = 3 ; interpolations = "linear" } :
                    { type = "Mean" ; meanFile = "$ConfigDir$/ImageNet1K_mean.xml" }
                )}
                labels = { labelDim = 1000 }
            }
        })
    }   
}